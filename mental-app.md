# Assistente Interno de Conhecimento ‚Äî Arquitetura RAG (Qdrant + Hybrid + HyDE)

## Objetivos do Assistente
Entregar respostas t√©cnicas, confi√°veis e citadas a partir de **milhares de PDFs**, **wikis privadas** e **tickets hist√≥ricos**, em **PT-BR/ES/EN**, com **baixa lat√™ncia**, **custos previs√≠veis** e **governan√ßa/seguran√ßa de n√≠vel corporativo**. O assistente deve: (1) **buscar** e **ranquear** trechos mais relevantes; (2) **gerar** respostas fundamentadas com **cita√ß√µes**; (3) respeitar **ACLs/tenant** e pol√≠ticas de **privacidade**; (4) operar em **cloud** ou **on-prem**; (5) ser **observ√°vel** e **tunic√°vel** (qualidade, custo, lat√™ncia).

---

## Como capturar, fatiar e indexar esse material em um banco vetorial.

### Ingest & Normaliza√ß√£o
O primeiro passo em qualquer aplica√ß√£o RAG √© **entender profundamente as necessidades do cliente**. Esse entendimento vem de uma comunica√ß√£o clara e direta: fazemos perguntas objetivas sobre **quais bases de conhecimento realmente importam** e **como est√£o estruturadas**.  

Imagine, por exemplo, um cliente do setor ambiental que mant√©m **milhares de PDFs de licenciamento** e **wikis internas** com guias t√©cnicos. O objetivo √© identificar **como esses documentos s√£o armazenados**, quais **regras de acesso** se aplicam e **qual granularidade de informa√ß√£o** deve ser recuperada.

A partir desse entendimento, desenhamos os fluxos de ingest√£o **separados por tipo de documento**:

- **PDFs digitais (nativos):** usar parsers *layout-aware* (Docling, LlamaParse, Unstructured) que preservam **hierarquia, tabelas e ordem de leitura**.  
- **PDFs escaneados:** aplicar OCR de alta qualidade (OCRmyPDF/Tesseract ou servi√ßos como Google Document AI, AWS Textract, Azure). Ap√≥s OCR, normalizar e dividir em se√ß√µes coerentes.  
- **Wikis (Confluence/MediaWiki):** extrair via **APIs oficiais**, expandindo corpo em HTML/XHTML, preservando **headings e anchors**. Fazer ingest√£o incremental (*delta-ingest*) com base em `lastModified`/revisions, respeitando ACLs.

Durante essa ingest√£o, garantimos **higiene dos dados** (deduplica√ß√£o via minhash, remo√ß√£o de boilerplate, mascaramento de PII) e anexamos **metadados ricos** (`tenant`, `client_id`, `doc_type`, `lang`, `created_at`, `effective_date`, `url#anchor`, `hash`) ‚Äî base essencial para auditoria e filtros r√≠gidos de seguran√ßa.

### Chunking (Qualidade antes de Quantidade)
Uma vez extra√≠dos e normalizados, os documentos precisam ser ‚Äúfatiados‚Äù em **chunks**, preservando o m√°ximo de contexto √∫til para o retriever. Esse processo tamb√©m √© adaptado **ao tipo de fonte**:

- **Padr√£o:** janelas recursivas de ~1.000 tokens com **10‚Äì15% de overlap**, garantindo continuidade entre trechos.  
- **PDFs extensos:** aplicar *semantic chunking*, agrupando frases ou par√°grafos por **similaridade sem√¢ntica**, o que respeita a l√≥gica do autor em documentos longos.  
- **Wikis estruturadas:** aplicar *header-aware splitting*, cortando por **se√ß√µes hier√°rquicas** (H1/H2/H3) e preservando contexto de **tabelas, listas e anchors**.  
- **Context-aware adicional:** enriquecer cada chunk com **1‚Äì2 linhas de resumo geradas por LLM**, ligando-o √† se√ß√£o-pai ou ao documento como um todo. Isso aumenta a precis√£o da recupera√ß√£o sem comprometer lat√™ncia (pr√©-processado).

### Indexa√ß√£o (Qdrant)
- **Vetores nomeados por ponto:**  
  - `dense_bge_m3` (embedding BGE-M3),  
  - `sparse_bm25/splade` (busca exata),  
  - opcional `colbert_tokens` (late-interaction) em cen√°rios de cl√°usulas cr√≠ticas.  
- **HNSW/ANN:** ajuste de `m` e `ef_construct`; em runtime, `ef` din√¢mico para calibrar **recall ‚Üî lat√™ncia**.  
- **Payload/ACL:** filtros r√≠gidos no servidor (tenant, ACL, idioma, `effective_date`).  
- **Escala & custo:** quantiza√ß√£o 8-bit das embeddings; *tiering* (√≠ndice em RAM, payload em disco/objeto); **multi-tenancy** por cole√ß√£o, garantindo isolamento e carregamento sob demanda.

---

## Qual modelo de embeddings e qual tipo de LLM faria sentido (e por qu√™).

### Embeddings (Multilinguagem e H√≠brido)
- **BGE-M3 como padr√£o √∫nico:** gera **denso + esparso + multi-vector**; cobre PT-BR/ES/EN e permite **retrieval h√≠brido** de alta qualidade. Simplifica arquitetura e abre caminho para **ColBERT** em dom√≠nios onde precis√£o cir√∫rgica √© essencial.  
- **Quantiza√ß√£o 8-bit:** reduz custo de mem√≥ria sem perda relevante de recall.  
- **Alternativas gerenciadas:** *Voyage-3-large* ou **OpenAI text-embedding-3-large** em contextos onde integra√ß√£o SaaS pr√©-existente ou benchmarks de precis√£o SOTA justificam.

### LLM Gerador (Cloud vs. On-Prem)
- **Cloud enterprise:** **GPT-4o** ou **Claude Sonnet** ‚Üí racioc√≠nio robusto, multil√≠ngue, seguran√ßa madura. Configura√ß√£o conservadora (`temperature‚â§0.5`, `top_p‚âà0.9`, *repetition penalty* leve). Sempre com **cita√ß√µes** verific√°veis.  
- **On-prem:** **Llama-3.1** (8B/70B ou 405B) quantizado para atender a requisitos de **soberania de dados**.  
- **Roteamento inteligente:** modelos de racioc√≠nio s√£o ativados apenas quando o **Router LLM** detecta alta complexidade (ex.: an√°lise de cl√°usulas ou pareceres t√©cnicos), controlando custo e lat√™ncia.

---

## Como as pe√ßas se conectam do ingest ao usu√°rio final (descreva passo a passo).

flowchart TD
    %% ============================================================
    %% Fluxo RAG ‚Äî do ingest ao usu√°rio (com Qdrant e busca h√≠brida)
    %% ============================================================

    %% 1) INGEST√ÉO & NORMALIZA√á√ÉO
    subgraph S1[1. Ingest√£o & Normaliza√ß√£o]
      A[Conectores<br/>PDFs ‚Ä¢ Wikis privadas ‚Ä¢ Tickets] --> B[Parsers / OCR]
      B --> C[Limpeza & De-dup]
      C --> D[PII Masking / Reda√ß√£o]
      D --> E[Metadados: t√≠tulo, autor, data, ACL, tenant]
    end

    %% 2) CHUNKING
    subgraph S2[2. Chunking]
      E --> F[Janela recursiva ‚Ä¢ overlap]
      F --> G[Semantic / Header-aware]
      G --> H[Resumo de contexto via LLM]
    end

    %% 3) INDEXA√á√ÉO EM QDRANT
    subgraph S3[3. Indexa√ß√£o em Qdrant]
      H --> I[Embeddings densos: bge-m3]
      H --> J[Esparsos: BM25 / SPLADE]
      I --> K[(Qdrant Collection)]
      J --> K
      K --> L[Config: HNSW ‚Ä¢ filtros ACL ‚Ä¢ quantiza√ß√£o]
    end

    %% 4‚Äì12) SERVING DA QUERY
    subgraph S4[4‚Äì12. Serving da Query]
      U[Usu√°rio] --> R[4. Router LLM<br/>decide necessidade de retrieval<br/>e tenant/cole√ß√£o]
      R -->|n√£o precisa| PG0[9. Montagem do Prompt (sem retrieval)]
      R -->|precisa| QP[5. Parsing da Query<br/>Reescrita (sin√¥nimos/normaliza√ß√£o) + HyDE]

      QP --> PF[6. Pr√©-filtro r√≠gido<br/>ACL ‚Ä¢ idioma ‚Ä¢ per√≠odo ‚Ä¢ doc_type]
      PF --> HB[7. Busca H√≠brida<br/>ANN denso (HNSW) + BM25/SPLADE<br/>Fus√£o RRF/pesos ‚Ä¢ Over-fetch K=30‚Äì100]
      HB --> RR[8. Re-rank<br/>cross-encoder (bge-reranker / Cohere)<br/>Seleciona Top 5‚Äì15]
      RR --> PG[9. Montagem do Prompt<br/>grounding + chunks cit√°veis + hist√≥rico + ferramentas]

      PG0 --> GEN
      PG  --> GEN
      GEN[10. Gera√ß√£o<br/>LLM (cloud ou on-prem)] --> PP[11. P√≥s-processamento<br/>valida√ß√£o JSON ‚Ä¢ compliance/reda√ß√£o ‚Ä¢ tradu√ß√£o/localiza√ß√£o]
      PP --> OUT[12. Entrega ao Usu√°rio<br/>resposta + cita√ß√µes clic√°veis]
    end

    %% 13) OBSERVABILIDADE
    subgraph S5[13. Observabilidade]
      OUT --> OBS[Coleta: lat√™ncia, recall@K, custo, erros de cobertura]
      OBS --> FB[Feedback do usu√°rio: üëç/üëé ‚Ä¢ coment√°rios]
      FB --> RT[IndexOps / Retuning<br/>re-embed ‚Ä¢ ajustar pesos RRF ‚Ä¢ HNSW]
      RT --> K
    end

    %% 14) RE-ITERA√á√ÉO (OPCIONAL)
    subgraph S6[14. (Opcional) Re-itera√ß√£o]
      DEC{Qualidade suficiente?}
      DEC -->|sim| END[(Fim)]
      DEC -->|n√£o| RE[Re-busca / ajuste de estrat√©gia]
      RE --> QP
      DEC -.-> HUM[Escalonamento humano<br/>(com contexto completo)]
      HUM -.-> OUT
    end

    OUT --> DEC

    %% (Estilo opcional)
    classDef db fill:#eef,stroke:#88a,stroke-width:1px;
    class K db


---

## Como voc√™ mediria custo, lat√™ncia e qualidade, al√©m de garantir seguran√ßa e controle de acesso.

### Qualidade
- **Retriever:** medir **Recall@K**, **Precision@K**, **MAP@K**, **MRR** em *golden sets* de prompts reais (10‚Äì20 por dom√≠nio).  
- **Gera√ß√£o:** avaliar **fidelidade √†s fontes** e **qualidade de cita√ß√µes** via *LLM-as-judge* + revis√£o humana em √°reas cr√≠ticas (jur√≠dico, sa√∫de).  
- **Sistema:** monitorar cobertura (‚â•1 chunk relevante), taxa de acerto percebida, thumbs up/down, escalonamentos.

### Lat√™ncia
- **Principais gargalos:** transformadores (gerador, re-ranker, query rewriter).  
- **Estrat√©gias:**  
  - roteamento para LLMs menores/quantizados em queries simples;  
  - limitar K (ex.: 50 ‚Üí top10);  
  - caching sem√¢ntico de perguntas frequentes;  
  - em Qdrant: √≠ndice HNSW em RAM, `ef` din√¢mico, sharding por tenant, vetores bin√°rios/8-bit para busca coarse + refine.

### Custo
- **LLMs:** modelos menores, *caps* de tokens, prompts enxutos, endpoints dedicados/on-prem conforme escala.  
- **Vetores:** quantiza√ß√£o, tiering (√≠ndice em RAM, payload em disco), multi-tenancy para carregamento seletivo; arquivamento de tenants inativos.  
- **Trade-off:** balancear over-fetch e re-rank; empiricamente, **K‚âà50 ‚Üí top10 re-rankeado** tende a √≥timo custo/benef√≠cio.

### Seguran√ßa & Controle de Acesso
- **Autentica√ß√£o/Autoriza√ß√£o:** SSO + RBAC/ABAC aplicados antes da busca e novamente antes do prompt.  
- **Isolamento:** cole√ß√µes por tenant em Qdrant, evitando vazamento entre clientes.  
- **Criptografia:** em tr√¢nsito e repouso; evitar envio de dados sens√≠veis a LLMs externos quando pol√≠tica restringir.  
- **Vetores como dados sens√≠veis:** acesso minimizado ao DB; hardening de endpoints; awareness do risco de reconstru√ß√£o textual.  
- **Pol√≠tica de gera√ß√£o:** *redaction* p√≥s-processamento, bloqueio de *jailbreaks*, rejei√ß√£o de afirma√ß√µes n√£o citadas.

---

## Porque essa Aplica√ß√£o se encaixa no FI Group
- **Ader√™ncia ao neg√≥cio:** conte√∫dos **regulat√≥rios e t√©cnicos** (incentivos √† inova√ß√£o, fiscal, energia, seguros) s√£o **multil√≠ngues** e **PDF-intensivos** ‚Äî o **h√≠brido denso+esparso** + **HyDE** maximiza **recall** sem abrir m√£o de precis√£o.  
- **Entrega r√°pida e segura:** **Qdrant** oferece **HNSW**, vetores nomeados, filtros por payload e **multi-tenancy**, facilitando **ACL forte**, isolamento por cliente e **escala el√°stica** (RAM/disco/objeto).  
- **Flexibilidade de implanta√ß√£o:** **GPT-4o/Claude** (SaaS) quando permitido; **Llama-3.1 on-prem** quando **soberania** e **compliance** exigirem. Troca de LLM √© **plug-and-play** (prompt e contratos est√°veis).  
- **Opera√ß√£o mensur√°vel:** m√©tricas de **qualidade/lat√™ncia/custo** integradas √† observabilidade (traces de ponta a ponta, *golden sets*, A/B). Time consegue iterar com seguran√ßa e provar **ROI**.  
- **Evolu√ß√£o orientada a risco:** come√ßar com **chunking recursivo + BGE-M3 + h√≠brido + re-rank**, e acionar **ColBERT** ou **modelos de racioc√≠nio** apenas onde o dom√≠nio exigir (jur√≠dico, m√©dico, auditoria), mantendo **TCO** sob controle.  
- **Valor ao cliente final:** respostas **citadas**, **consistentes** e **audit√°veis**, integradas ao ecossistema FI Group, acelerando an√°lises, reduzindo retrabalho e elevando qualidade de entreg√°veis.

> **Resumo**: uma √∫nica arquitetura **RAG** moderna (Qdrant + **BGE-M3** + h√≠brido **ANN/BM25** + **HyDE** + re-rank) com **governan√ßa**, **medidas claras** e **op√ß√µes de implanta√ß√£o** que atendem rapidamente os diferentes cen√°rios do FI Group ‚Äî do prot√≥tipo √† produ√ß√£o.
